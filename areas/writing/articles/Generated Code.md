Very often a code component will appear well tested and documented, especially for busy coders that don't have time to read a README file for no reason, or to run tests on code not related to their projects. However, sometimes a closer look at the code base reveals code that should have received more attention than it did.

Often developers will accept defaults, borrow a code snippet, or generate some tests, just to get back to really important work without delay. There is a crossover point though, where these hastily conjured pieces of code cease to contribute to code quality and become liabilities. Tests that only have coverage but no depth, documentation that is either fully outdated, or never even properly developed, and methods that only say, "Hello world".

Shallow or meaningless code satisfies build pipeline checks that don't actually look at the code, but just check if it exists. A build tool that recognizes well known template code, example code, and maybe even generated tests, would be a great addition to code quality tool sets. Our immediate focus is on code documentation, so we needn't be up to analyzing tests, and we can focus on comparing code documentation artifacts. 

Documentation artifacts are normally plain text files or close, like XML, HTML or (hopefully) MarkDown. These are very easy to compare manually, a quick scan can give a human reader a good overview of what the text covers, but until fairly recently, it has been quite difficult to analyze the content of natural language text documentation, and even more difficult to check for plagiarism. 

That is, until the advent of readily available and well trained ML and AI models, especially Large  Language Models, often trained on swathes of natural language documentation. Someone recently said, of a well known model, that it had already, "read the internet". One of the first applications of such models was, naturally, plagiarism, and soon after that, plagiarism checkers.

This brings me to the main point of this project, which is to look at ways readily available LLMs can be used to analyze code documentation with the goal of recognizing generated or copied documentation during CI/CD pipelines. Services available now can even also for code generated by AI tools, and although that leaves many questions unanswered, as long as the code works as intended, it is not our immediate concern who or what created it.

